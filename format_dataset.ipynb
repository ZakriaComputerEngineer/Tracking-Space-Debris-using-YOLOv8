{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd18uzRYEs6l",
        "outputId": "4513bfcd-0f85-485a-ef8f-cb70a1029eef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFN1o8IAF7K0",
        "outputId": "f364eb9f-9bf9-4803-e127-5432a84bea7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n",
            "Architecture:             x86_64\n",
            "  CPU op-mode(s):         32-bit, 64-bit\n",
            "  Address sizes:          46 bits physical, 48 bits virtual\n",
            "  Byte Order:             Little Endian\n",
            "CPU(s):                   2\n",
            "  On-line CPU(s) list:    0,1\n",
            "Vendor ID:                GenuineIntel\n",
            "  Model name:             Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "    CPU family:           6\n",
            "    Model:                79\n",
            "    Thread(s) per core:   2\n",
            "    Core(s) per socket:   1\n",
            "    Socket(s):            1\n",
            "    Stepping:             0\n",
            "    BogoMIPS:             4399.99\n",
            "    Flags:                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 cl\n",
            "                          flush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc re\n",
            "                          p_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3\n",
            "                           fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand\n",
            "                           hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp \n",
            "                          fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx sm\n",
            "                          ap xsaveopt arat md_clear arch_capabilities\n",
            "Virtualization features:  \n",
            "  Hypervisor vendor:      KVM\n",
            "  Virtualization type:    full\n",
            "Caches (sum of all):      \n",
            "  L1d:                    32 KiB (1 instance)\n",
            "  L1i:                    32 KiB (1 instance)\n",
            "  L2:                     256 KiB (1 instance)\n",
            "  L3:                     55 MiB (1 instance)\n",
            "NUMA:                     \n",
            "  NUMA node(s):           1\n",
            "  NUMA node0 CPU(s):      0,1\n",
            "Vulnerabilities:          \n",
            "  Gather data sampling:   Not affected\n",
            "  Itlb multihit:          Not affected\n",
            "  L1tf:                   Mitigation; PTE Inversion\n",
            "  Mds:                    Vulnerable; SMT Host state unknown\n",
            "  Meltdown:               Vulnerable\n",
            "  Mmio stale data:        Vulnerable\n",
            "  Reg file data sampling: Not affected\n",
            "  Retbleed:               Vulnerable\n",
            "  Spec rstack overflow:   Not affected\n",
            "  Spec store bypass:      Vulnerable\n",
            "  Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swa\n",
            "                          pgs barriers\n",
            "  Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BH\n",
            "                          I: Vulnerable (Syscall hardening enabled)\n",
            "  Srbds:                  Not affected\n",
            "  Tsx async abort:        Vulnerable\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "!lscpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H1CR6fVqGhNx"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import math\n",
        "import pandas as pd\n",
        "import cv2  # OpenCV for image processing\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import ast\n",
        "from tqdm import tqdm\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Et-8EpT1F40Y"
      },
      "outputs": [],
      "source": [
        "def extract_zip(zip_file_path, output_folder):\n",
        "    # Create the output folder if it doesn't exist\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Open the zip file\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        # Extract all the contents into the output folder\n",
        "        zip_ref.extractall(output_folder)\n",
        "\n",
        "# Example usage\n",
        "zip_file_path = '/content/drive/MyDrive/Copy of val.zip'  # Replace with your zip file path\n",
        "output_folder = '/content/drive/MyDrive/copy of val (unzipped)'     # Replace with your desired output folder path\n",
        "\n",
        "extract_zip(zip_file_path, output_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OvIwtHnH9gG"
      },
      "outputs": [],
      "source": [
        "def get_directory_info(directory_path):\n",
        "    total_size = 0\n",
        "    num_files = 0\n",
        "\n",
        "    for dirpath, dirnames, filenames in os.walk(directory_path):\n",
        "        for file in filenames:\n",
        "            file_path = os.path.join(dirpath, file)\n",
        "            total_size += os.path.getsize(file_path)\n",
        "            num_files += 1\n",
        "\n",
        "    return num_files, total_size\n",
        "\n",
        "def convert_size(size_bytes):\n",
        "    if size_bytes == 0:\n",
        "        return \"0B\"\n",
        "    size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\")\n",
        "    i = int(math.floor(math.log(size_bytes, 1024)))\n",
        "    p = math.pow(1024, i)\n",
        "    s = round(size_bytes / p, 2)\n",
        "    return f\"{s} {size_name[i]}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-MbfQSWZnmA",
        "outputId": "62ffe04f-ffc0-46df-82ca-c68ab87db56f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading data: 11000it [20:43,  8.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of files loaded: 11000\n",
            "Dataset prepared in YOLOv5 format saved in: /content/drive/MyDrive/dataset_yolov5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Define class mapping (this should be based on the classes you have in your dataset)\n",
        "class_mapping = {\n",
        "    'lisa_pathfinder': 0,\n",
        "    'proba_3_csc': 1,\n",
        "    'smart_1': 2,\n",
        "    'xmm_newton': 3,\n",
        "    'soho': 4,\n",
        "    'earth_observation_sat_1': 5,\n",
        "    'debris': 6,\n",
        "    'proba_2': 7,\n",
        "    'proba_3_ocs': 8,\n",
        "    'cheops': 9,\n",
        "    'double_star': 10\n",
        "}\n",
        "\n",
        "def convert_bbox(size, box):\n",
        "    \"\"\"\n",
        "    Convert bounding box to YOLO format\n",
        "    \"\"\"\n",
        "    dw = 1. / size[0]\n",
        "    dh = 1. / size[1]\n",
        "    x = (box[0] + box[2]) / 2.0\n",
        "    y = (box[1] + box[3]) / 2.0\n",
        "    w = box[2] - box[0]\n",
        "    h = box[3] - box[1]\n",
        "    x = x * dw\n",
        "    w = w * dw\n",
        "    y = y * dh\n",
        "    h = h * dh\n",
        "    return (x, y, w, h)\n",
        "\n",
        "def load_data(csv_file, images_folder, output_folder, start_index, test_size=0.1):\n",
        "    df = pd.read_csv(csv_file)\n",
        "    df = df.iloc[start_index:start_index + len(df) // 2]  # Use only 50% of the CSV file\n",
        "    loaded_count = start_index\n",
        "\n",
        "    for index, row in tqdm(df.iterrows(), desc=\"Loading data\"):\n",
        "        if index < start_index:\n",
        "            continue\n",
        "        # Replace .png with .jpg in image filename\n",
        "        image_name = row['filename'].replace('.png', '.jpg')\n",
        "        image_path = os.path.join(images_folder, image_name)\n",
        "\n",
        "        if not os.path.exists(image_path):\n",
        "            print(f\"Warning: File '{image_path}' not found. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        if image is None:\n",
        "            print(f\"Warning: Unable to load image '{image_path}'. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # YOLO typically uses RGB images\n",
        "\n",
        "        bbox = ast.literal_eval(row['bbox'])\n",
        "        class_label = row['class']\n",
        "\n",
        "        if class_label not in class_mapping:\n",
        "            print(f\"Warning: Class label '{class_label}' not in class mapping. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        class_id = class_mapping[class_label]\n",
        "        yolo_bbox = convert_bbox(image.shape[:2], bbox)  # Convert bbox to YOLO format\n",
        "\n",
        "        # YOLO format: [<class_index> <x_center> <y_center> <width> <height>]\n",
        "        annotation = f\"{class_id} {' '.join(map(str, yolo_bbox))}\"\n",
        "\n",
        "        loaded_count += 1\n",
        "\n",
        "        # Save the image and annotation in the YOLOv5 format\n",
        "        subset_folder = 'train' if np.random.rand() > test_size else 'val'\n",
        "        output_image_path = os.path.join(output_folder, subset_folder, 'images', image_name)\n",
        "        output_label_path = os.path.join(output_folder, subset_folder, 'labels', image_name.replace('.jpg', '.txt'))\n",
        "\n",
        "        os.makedirs(os.path.dirname(output_image_path), exist_ok=True)\n",
        "        os.makedirs(os.path.dirname(output_label_path), exist_ok=True)\n",
        "\n",
        "        cv2.imwrite(output_image_path, image)\n",
        "        with open(output_label_path, 'w') as f:\n",
        "            f.write(annotation)\n",
        "\n",
        "    print(f\"Number of files loaded: {loaded_count}\")\n",
        "\n",
        "    return output_folder\n",
        "\n",
        "# Example usage\n",
        "val_csv_path = '/content/drive/MyDrive/Copy of labels.zip (Unzipped Files)/val.csv'  # Path to your val.csv file\n",
        "images_folder = '/content/drive/MyDrive/copy of val (unzipped)/val'  # Folder where images are stored\n",
        "output_folder = '/content/drive/MyDrive/dataset_yolov5'  # Output folder where YOLOv5 formatted dataset will be saved\n",
        "\n",
        "# Load and prepare the dataset\n",
        "dataset_folder = load_data(val_csv_path, images_folder, output_folder, start_index=0)\n",
        "print(f\"Dataset prepared in YOLOv5 format saved in: {dataset_folder}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKdOCIrTybCw",
        "outputId": "98e3763e-ee95-4489-c6a3-ff9ca9638288"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading train data: 150it [00:16,  9.01it/s]\n",
            "Loading val data: 50it [00:05,  8.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset prepared in YOLOv5 format saved in: /content/drive/MyDrive/dataset2_yolov5\n"
          ]
        }
      ],
      "source": [
        "class_mapping = {\n",
        "    'lisa_pathfinder': 0,\n",
        "    'proba_3_csc': 1,\n",
        "    'smart_1': 2,\n",
        "    'xmm_newton': 3,\n",
        "    'soho': 4,\n",
        "    'earth_observation_sat_1': 5,\n",
        "    'debris': 6,\n",
        "    'proba_2': 7,\n",
        "    'proba_3_ocs': 8,\n",
        "    'cheops': 9,\n",
        "    'double_star': 10\n",
        "}\n",
        "\n",
        "def normalize_bbox(image_shape, bbox):\n",
        "    \"\"\"\n",
        "    Normalize bounding box coordinates to [0, 1] range\n",
        "    \"\"\"\n",
        "    height, width = image_shape\n",
        "    x_min, y_min, x_max, y_max = bbox\n",
        "    x_min /= width\n",
        "    x_max /= width\n",
        "    y_min /= height\n",
        "    y_max /= height\n",
        "    return [x_min, y_min, x_max, y_max]\n",
        "\n",
        "def load_data(csv_file, images_folder, output_folder):\n",
        "    df = pd.read_csv(csv_file)\n",
        "    train_df = df.iloc[:150]\n",
        "    val_df = df.iloc[150:200]\n",
        "\n",
        "    for df, subset_folder in zip([train_df, val_df], ['train', 'val']):\n",
        "        for index, row in tqdm(df.iterrows(), desc=f\"Loading {subset_folder} data\"):\n",
        "            # Replace .png with .jpg in image filename\n",
        "            image_name = row['filename'].replace('.png', '.jpg')\n",
        "            image_path = os.path.join(images_folder, image_name)\n",
        "\n",
        "            if not os.path.exists(image_path):\n",
        "                print(f\"Warning: File '{image_path}' not found. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            image = cv2.imread(image_path)\n",
        "\n",
        "            if image is None:\n",
        "                print(f\"Warning: Unable to load image '{image_path}'. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            bbox = ast.literal_eval(row['bbox'])\n",
        "            class_label = row['class']\n",
        "\n",
        "            if class_label not in class_mapping:\n",
        "                print(f\"Warning: Class label '{class_label}' not in class mapping. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            class_id = class_mapping[class_label]\n",
        "            normalized_bbox = normalize_bbox(image.shape[:2], bbox)\n",
        "\n",
        "            if any(coord < 0 or coord > 1 for coord in normalized_bbox):\n",
        "                print(f\"Warning: Normalized bbox out of bounds for image '{image_name}'. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            bbox_str = ' '.join(map(str, normalized_bbox))\n",
        "\n",
        "            # YOLO format: [<class_index> <x_min> <y_min> <x_max> <y_max>]\n",
        "            annotation = f\"{class_id} {bbox_str}\"\n",
        "\n",
        "            # Save the image and annotation in the YOLOv5 format\n",
        "            output_image_path = os.path.join(output_folder, subset_folder, 'images', image_name)\n",
        "            output_label_path = os.path.join(output_folder, subset_folder, 'labels', image_name.replace('.jpg', '.txt'))\n",
        "\n",
        "            os.makedirs(os.path.dirname(output_image_path), exist_ok=True)\n",
        "            os.makedirs(os.path.dirname(output_label_path), exist_ok=True)\n",
        "\n",
        "            cv2.imwrite(output_image_path, image)\n",
        "            with open(output_label_path, 'w') as f:\n",
        "                f.write(annotation)\n",
        "\n",
        "    print(f\"Dataset prepared in YOLOv5 format saved in: {output_folder}\")\n",
        "\n",
        "# Example usage\n",
        "csv_path = '/content/drive/MyDrive/Copy of labels.zip (Unzipped Files)/val.csv'  # Path to your val.csv file\n",
        "images_folder = '/content/drive/MyDrive/copy of val (unzipped)/val'  # Folder where images are stored\n",
        "output_folder = '/content/drive/MyDrive/dataset2_yolov5'  # Output folder where YOLOv5 formatted dataset will be saved\n",
        "\n",
        "# Load and prepare the dataset\n",
        "load_data(csv_path, images_folder, output_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3mQlJKD-Cio",
        "outputId": "3d1608ff-4d54-4aa3-b723-f708bbf94cc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created YAML file: /content/drive/MyDrive/Copy of labels.zip (Unzipped Files)/dataset.yaml\n"
          ]
        }
      ],
      "source": [
        "def create_yaml_config(csv_file, images_folder, output_yaml):\n",
        "    # Read CSV file to get unique classes\n",
        "    df = pd.read_csv(csv_file)\n",
        "    classes = df['class'].unique().tolist()\n",
        "\n",
        "    # Create YAML content\n",
        "    yaml_content = f\"\"\"\n",
        "    train: {os.path.join(images_folder, 'train', 'images')}\n",
        "    val: {os.path.join(images_folder, 'val', 'images')}\n",
        "\n",
        "    nc: {len(classes)}  # number of classes\n",
        "    names: {classes}  # class names\n",
        "    \"\"\"\n",
        "\n",
        "    # Write YAML file\n",
        "    with open(output_yaml, 'w') as f:\n",
        "        f.write(yaml_content)\n",
        "\n",
        "    print(f\"Created YAML file: {output_yaml}\")\n",
        "\n",
        "# Example usage\n",
        "csv_file = '/content/drive/MyDrive/Copy of labels.zip (Unzipped Files)/val.csv'\n",
        "images_folder = '/content/drive/MyDrive/dataset_yolov5'\n",
        "output_yaml = '/content/drive/MyDrive/Copy of labels.zip (Unzipped Files)/dataset.yaml'\n",
        "\n",
        "create_yaml_config(csv_file, images_folder, output_yaml)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsmKZx3-gGBI",
        "outputId": "74ad2c6c-9b8f-45a2-d1f8-88418757b461"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of files in training: 9840\n",
            "Total size: 3.32 GB\n",
            "Number of files in validation: 1160\n",
            "Total size: 398.02 MB\n"
          ]
        }
      ],
      "source": [
        "#num_files, total_size = get_directory_info('/content/drive/MyDrive/copy of val (unzipped)/val')\n",
        "#print(f\"Number of files in unzipped folder: {num_files}\")\n",
        "#print(f\"Total size: {convert_size(total_size)}\")\n",
        "\n",
        "num_files, total_size = get_directory_info('/content/drive/MyDrive/dataset_yolov5/train/images')\n",
        "print(f\"Number of files in training: {num_files}\")\n",
        "print(f\"Total size: {convert_size(total_size)}\")\n",
        "\n",
        "num_files, total_size = get_directory_info('/content/drive/MyDrive/dataset_yolov5/val/images')\n",
        "print(f\"Number of files in validation: {num_files}\")\n",
        "print(f\"Total size: {convert_size(total_size)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "NRcAUgTDh4JS",
        "outputId": "5602cdc9-63f4-415d-a857-9fb41c0334e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scanning images:   8%|â–Š         | 1603/19736 [37:55<7:09:04,  1.42s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-78ab4328f544>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mtrain_labels_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/dataset_yolov5/train/labels'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mcorrupt_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_corrupt_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Found {len(corrupt_images)} corrupt images.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-78ab4328f544>\u001b[0m in \u001b[0;36mfind_corrupt_images\u001b[0;34m(image_folder, label_folder)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mcorrupt_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def find_corrupt_images(image_folder, label_folder):\n",
        "    corrupt_images = []\n",
        "\n",
        "    for img_file in tqdm(os.listdir(image_folder), desc=\"Scanning images\"):\n",
        "        img_path = os.path.join(image_folder, img_file)\n",
        "        label_path = os.path.join(label_folder, img_file.replace('.jpg', '.txt'))\n",
        "\n",
        "        try:\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                corrupt_images.append(img_path)\n",
        "                print(f'{img_file} img doesnt exist')\n",
        "                continue\n",
        "\n",
        "            # Check if label file exists\n",
        "            if not os.path.exists(label_path):\n",
        "                corrupt_images.append(img_path)\n",
        "                print(f'{img_file} label doesnt exist')\n",
        "                continue\n",
        "\n",
        "            # Additional checks for label file (optional)\n",
        "            with open(label_path, 'r') as f:\n",
        "                label_content = f.read()\n",
        "                if not label_content.strip():\n",
        "                    corrupt_images.append(img_path)\n",
        "                    print(f'{img_file} labels empty')\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            corrupt_images.append(img_path)\n",
        "\n",
        "    return corrupt_images\n",
        "\n",
        "train_images_folder = '/content/drive/MyDrive/dataset_yolov5/train/images'\n",
        "train_labels_folder = '/content/drive/MyDrive/dataset_yolov5/train/labels'\n",
        "\n",
        "corrupt_images = find_corrupt_images(train_images_folder, train_labels_folder)\n",
        "print(f\"Found {len(corrupt_images)} corrupt images.\")\n",
        "\n",
        "# Optionally, save the list of corrupt images to a file\n",
        "#with open('corrupt_images.txt', 'w') as f:\n",
        "#    for img in corrupt_images:\n",
        "#        f.write(f\"{img}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8iPf4ACWDCB",
        "outputId": "afada3ca-75fb-41c8-fde2-fe8c2bcbb353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/yolov5\n",
            "2024-06-25 14:54:01.163877: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-25 14:54:01.163950: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-25 14:54:01.166820: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=/content/drive/MyDrive/Copy of labels.zip (Unzipped Files)/yolov5s.pt, cfg=, data=/content/dataset.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=100, batch_size=32, imgsz=512, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
            "YOLOv5 ðŸš€ v7.0-331-gab364c98 Python-3.10.12 torch-2.3.0+cu121 CPU\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 ðŸš€ runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Overriding model.yaml nc=80 with nc=11\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     43152  models.yolo.Detect                      [11, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7049296 parameters, 7049296 gradients, 16.0 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from /content/drive/MyDrive/Copy of labels.zip (Unzipped Files)/yolov5s.pt\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/dataset2_yolov5/train/labels... 150 images, 0 backgrounds, 0 corrupt: 100% 150/150 [00:02<00:00, 64.23it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/dataset2_yolov5/train/labels.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/dataset2_yolov5/val/labels... 50 images, 0 backgrounds, 0 corrupt: 100% 50/50 [00:01<00:00, 48.40it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/dataset2_yolov5/val/labels.cache\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m2.94 anchors/target, 0.993 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
            "Plotting labels to runs/train/exp2/labels.jpg... \n",
            "Image sizes 512 train, 512 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp2\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       0/99         0G     0.1172    0.02689    0.07476         70        512:  40% 2/5 [02:06<03:09, 63.06s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/yolov5/train.py\", line 851, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/yolov5/train.py\", line 626, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"/content/yolov5/train.py\", line 385, in train\n",
            "    pred = model(imgs)  # forward\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/yolov5/models/yolo.py\", line 267, in forward\n",
            "    return self._forward_once(x, profile, visualize)  # single-scale inference, train\n",
            "  File \"/content/yolov5/models/yolo.py\", line 167, in _forward_once\n",
            "    x = m(x)  # run\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/yolov5/models/common.py\", line 238, in forward\n",
            "    return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/yolov5/models/common.py\", line 86, in forward\n",
            "    return self.act(self.bn(self.conv(x)))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 460, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 456, in _conv_forward\n",
            "    return F.conv2d(input, weight, bias, self.stride,\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "# Change directory to the YOLOv5 repository\n",
        "%cd /content/yolov5\n",
        "\n",
        "# Train the model with the specified parameters\n",
        "!python train.py --img 512 --batch 32 --epochs 100 --data '/content/dataset.yaml' --weights '/content/drive/MyDrive/Copy of labels.zip (Unzipped Files)/yolov5s.pt'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sh3MchP1Ze-t"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
